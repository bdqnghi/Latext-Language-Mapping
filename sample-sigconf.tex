\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{listings}    
\usepackage{subcaption}
\usepackage{multirow}
% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}





\begin{document}
\title{Learning Cross Language Mapping through Distributed Vector Representation}


\author{Nghi Bui}

\affiliation{%
  \department{School of Information Systems}
  \institution{Singapore Management University}
}
\email{dqnbui.2016@phdis.smu.edu.sg}

\author{Lingxiao Jiang}
\affiliation{%
  \department{School of Information Systems}
  \institution{Singapore Management University}
}
\email{lxjiang@smu.edu.sg}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{Nghi et al.}


\begin{abstract}
Mapping an elements from a program written in a language to another is a common software engineering task. In this paper, we propose a novel approach to find such mappings by leveraging Word2Vec, a neural-based technique to learn vector representation of word in natural language. Word2Vec, aka word embeddings, is a class of neural network models that is being trained from a large corpus of texts, they can produce for each unique word a corresponding vector in a continuous space in which linguistic contexts of words can be observed. In this work, instead of using word2vec for natural languages, which have been studied extensively, we use it for programming languages. The key contribution in this paper is that we apply Word2Vec to learn a shared embeddings crosslingually by introducing a step to add more semantic information into the training corpus. By leveraging the files with similar name in different language, we can get a parallel corpus, so called aligned files, then we can learn shared embeddings from such alignments. From the share embeddings, we can map the similar elements between languages accurately. Our empirical evaluation shows that our model can be used to map elements between languages accurately with high Mean Average Precision score. We also evaluate on a well-known real world task, the clone detection, we achieve a stable precision 84.2\% and recall 85.23\%.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>  
\end{CCSXML}




\keywords{software mining, text mining}


\maketitle

\input{samplebody-conf}

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-bibliography} 

\end{document}
